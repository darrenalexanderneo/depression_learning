{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Submission 5.2 - Text Classification with Deep Learning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W4RAfFaHPviy"},"source":["# Import necessary libraries\n","\n","<b> Note that </b>: If you're using Jupyter Notebook, you wouldn't need the drive.mount(\"/content/drive\")."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6hIhC0tHKEs","executionInfo":{"status":"ok","timestamp":1636861886934,"user_tz":-480,"elapsed":20654,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"8b3bb4db-3a2e-4a27-fdce-e9b8f81d521f"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras import layers\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support\n","import gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"u06SvU--P-_m"},"source":["<b> Note that: </b> If you're using a local directory, you'll have to read_excel from the appropriate directory."]},{"cell_type":"code","metadata":{"id":"mJghpKMjHRNh"},"source":["df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/IS424 Depression Project/model_data_v2.xlsx\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WZc4VCeHoVs"},"source":["# This step is to drop unnecessary columns, and empty rows.\n","\n","df = df.rename(columns={\"text_cleaned\": \"text\"})\n","df = df.drop(\"Unnamed: 0\", axis=1)\n","df['text'] = df['text'].astype(\"str\")\n","df = df.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBu7J8cbH5Yt","executionInfo":{"status":"ok","timestamp":1636862021239,"user_tz":-480,"elapsed":1804,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"668324de-5684-4fc6-ffa1-97a3b88b2c09"},"source":["# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 50000\n","\n","# Max number of words in each thread. For this one, we just used the max length of word of all sentences in the data.\n","MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in df['text']])\n","\n","# Hyperparameter. We tested with different parameters and 128 returns the best results.\n","EMBEDDING_DIM = 128\n","\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(df['text'].values)\n","word_index = tokenizer.word_index\n","\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 53660 unique tokens.\n"]}]},{"cell_type":"code","metadata":{"id":"caaPcM8GwM7n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636862022956,"user_tz":-480,"elapsed":1721,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"455e9a3d-1b0a-4d01-f337-8270bca19926"},"source":["X = tokenizer.texts_to_sequences(df['text'].values)\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', X.shape)\n","\n","Y = pd.get_dummies(df['depression']).values\n","print('Shape of label tensor:', Y.shape)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (38362, 2521)\n","Shape of label tensor: (38362, 2)\n","(26853, 2521) (26853, 2)\n","(11509, 2521) (11509, 2)\n"]}]},{"cell_type":"code","metadata":{"id":"LcdLuWJz_xR9"},"source":["# This is for testing later. Y_test_arr stores the original classes' outputs in a 1D array.\n","Y_test_arr = []\n","for idx in range(len(Y_test)):\n","  if Y_test[idx][0] == 1:\n","    Y_test_arr.append(0)\n","  else:\n","    Y_test_arr.append(1) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qk5mOYWUwIxh"},"source":["# LSTM + Keras' Word Embeddings\n"]},{"cell_type":"code","metadata":{"id":"xHbExUQ8wIM7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635324805549,"user_tz":-480,"elapsed":586099,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"92552386-8aa0-48a1-edf9-ed43df979b20"},"source":["|# LSTM\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 86s 283ms/step - loss: 0.3894 - accuracy: 0.9064 - val_loss: 0.2323 - val_accuracy: 0.9506\n","Epoch 2/25\n","294/294 [==============================] - 82s 280ms/step - loss: 0.1659 - accuracy: 0.9666 - val_loss: 0.1710 - val_accuracy: 0.9600\n","Epoch 3/25\n","294/294 [==============================] - 83s 283ms/step - loss: 0.1236 - accuracy: 0.9786 - val_loss: 0.1623 - val_accuracy: 0.9610\n","Epoch 4/25\n","294/294 [==============================] - 84s 285ms/step - loss: 0.1004 - accuracy: 0.9847 - val_loss: 0.1578 - val_accuracy: 0.9614\n","Epoch 5/25\n","294/294 [==============================] - 84s 285ms/step - loss: 0.0869 - accuracy: 0.9882 - val_loss: 0.1629 - val_accuracy: 0.9597\n","Epoch 6/25\n","294/294 [==============================] - 84s 284ms/step - loss: 0.0745 - accuracy: 0.9905 - val_loss: 0.1596 - val_accuracy: 0.9594\n","Epoch 7/25\n","294/294 [==============================] - 83s 284ms/step - loss: 0.0650 - accuracy: 0.9922 - val_loss: 0.1593 - val_accuracy: 0.9593\n"]}]},{"cell_type":"code","metadata":{"id":"C4e-fHmcwIPg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635325116357,"user_tz":-480,"elapsed":82595,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"8065aed9-260e-46f5-c5d0-b89f137edb10"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 42s 116ms/step - loss: 0.1578 - accuracy: 0.9615\n","Test set\n","  Loss: 0.158\n","  Accuracy: 0.962\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FX094_HF90EX","executionInfo":{"status":"ok","timestamp":1635325748491,"user_tz":-480,"elapsed":458,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"7430e8ff-f1eb-41c5-c8ba-a80f43b2406f"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision is:  0.9615153440042628\n","Recall is:  0.9615083847423755\n","F1 Score is:  0.9615106347249784\n"]}]},{"cell_type":"markdown","metadata":{"id":"2lQDdDSOwd77"},"source":["# Bi-LSTM + Keras' Word Embeddings"]},{"cell_type":"code","metadata":{"id":"1u8NANiswIRH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635327454505,"user_tz":-480,"elapsed":1673470,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"e349eccb-a4b2-49b6-d9ce-bfe038037ef6"},"source":["#Bi-LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(layers.Embedding(len(word_index)+1, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001))))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 171s 569ms/step - loss: 0.3986 - accuracy: 0.9105 - val_loss: 0.2091 - val_accuracy: 0.9633\n","Epoch 2/25\n","294/294 [==============================] - 165s 562ms/step - loss: 0.1612 - accuracy: 0.9743 - val_loss: 0.1752 - val_accuracy: 0.9647\n","Epoch 3/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1186 - accuracy: 0.9843 - val_loss: 0.1655 - val_accuracy: 0.9641\n","Epoch 4/25\n","294/294 [==============================] - 167s 570ms/step - loss: 0.0929 - accuracy: 0.9887 - val_loss: 0.1658 - val_accuracy: 0.9605\n","Epoch 5/25\n","294/294 [==============================] - 168s 571ms/step - loss: 0.0837 - accuracy: 0.9903 - val_loss: 0.1587 - val_accuracy: 0.9628\n","Epoch 6/25\n","294/294 [==============================] - 167s 569ms/step - loss: 0.0751 - accuracy: 0.9922 - val_loss: 0.1518 - val_accuracy: 0.9641\n","Epoch 7/25\n","294/294 [==============================] - 166s 566ms/step - loss: 0.0611 - accuracy: 0.9949 - val_loss: 0.1496 - val_accuracy: 0.9643\n","Epoch 8/25\n","294/294 [==============================] - 168s 572ms/step - loss: 0.0550 - accuracy: 0.9949 - val_loss: 0.1538 - val_accuracy: 0.9600\n","Epoch 9/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.0488 - accuracy: 0.9958 - val_loss: 0.1585 - val_accuracy: 0.9604\n","Epoch 10/25\n","294/294 [==============================] - 165s 563ms/step - loss: 0.0447 - accuracy: 0.9957 - val_loss: 0.1850 - val_accuracy: 0.9494\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emNSfQNIC6zi","executionInfo":{"status":"ok","timestamp":1635327774274,"user_tz":-480,"elapsed":284523,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"7bf603f7-a3c7-433d-9c60-e6674b9ccdd9"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 85s 236ms/step - loss: 0.1608 - accuracy: 0.9579\n","Test set\n","  Loss: 0.161\n","  Accuracy: 0.958\n","Precision is:  0.9582008817589148\n","Recall is:  0.9579459553392997\n","F1 Score is:  0.9579097474929152\n"]}]},{"cell_type":"markdown","metadata":{"id":"rCWBQWAPwlom"},"source":["# LSTM + CBOW\n","\n","- Choose where you want to save the embedding_word2vec txt files.\n","- You will be retrieving from that directory in the next code!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"528OkXhrxAJg","executionInfo":{"status":"ok","timestamp":1635327845520,"user_tz":-480,"elapsed":42072,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"9af70b3d-c906-4c7d-a8e1-f11a3c05a2ca"},"source":["\"\"\"\n","CBOW Model\n","\"\"\"\n","text_sentences = df['text'].apply(lambda x: x.split())\n","\n","model = gensim.models.Word2Vec(sentences=text_sentences, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","# Where you want to save.\n","filename = 'depression_embedding_word2vec.txt'\n","model.wv.save_word2vec_format(filename, binary=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53756\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPGzYC6r3ntf","executionInfo":{"status":"ok","timestamp":1635327859197,"user_tz":-480,"elapsed":6686,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"cc484bb2-0a07-4188-91c9-2476ca99c09a"},"source":["import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/depression_embedding_word2vec.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","word2vec_embedding_matrix = np.zeros((num_words, 128))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    word2vec_embedding_matrix[i] = embedding_vector \n","print(num_words)\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'].values)\n","X = tokenizer.texts_to_sequences(df['text'].values)\n","\n","word_index = tokenizer.word_index\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","Y = pd.get_dummies(df['depression']).values\n","print('Shape of label tensor:', Y.shape)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 99)\n","# X_train, X_test, Y_train, Y_test = train_test_split(X_svd, Y, test_size = 0.30, random_state = 42)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53661\n","Shape of label tensor: (38362, 2)\n","(26853, 2521) (26853, 2)\n","(11509, 2521) (11509, 2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKchG2G63nv-","executionInfo":{"status":"ok","timestamp":1635329517566,"user_tz":-480,"elapsed":1636742,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"89f57041-68ab-401a-ccb4-fd79f3eb939f"},"source":["# CBOW LSTM\n","\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 128, weights=[word2vec_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 82s 271ms/step - loss: 0.3951 - accuracy: 0.9046 - val_loss: 0.2403 - val_accuracy: 0.9490\n","Epoch 2/25\n","294/294 [==============================] - 78s 267ms/step - loss: 0.2264 - accuracy: 0.9471 - val_loss: 0.1947 - val_accuracy: 0.9575\n","Epoch 3/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1996 - accuracy: 0.9508 - val_loss: 0.1792 - val_accuracy: 0.9571\n","Epoch 4/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1827 - accuracy: 0.9539 - val_loss: 0.1751 - val_accuracy: 0.9563\n","Epoch 5/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1729 - accuracy: 0.9541 - val_loss: 0.1748 - val_accuracy: 0.9535\n","Epoch 6/25\n","294/294 [==============================] - 78s 265ms/step - loss: 0.1657 - accuracy: 0.9555 - val_loss: 0.1562 - val_accuracy: 0.9602\n","Epoch 7/25\n","294/294 [==============================] - 78s 265ms/step - loss: 0.1603 - accuracy: 0.9563 - val_loss: 0.1566 - val_accuracy: 0.9590\n","Epoch 8/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1590 - accuracy: 0.9562 - val_loss: 0.1539 - val_accuracy: 0.9599\n","Epoch 9/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1539 - accuracy: 0.9564 - val_loss: 0.1573 - val_accuracy: 0.9573\n","Epoch 10/25\n","294/294 [==============================] - 78s 265ms/step - loss: 0.1525 - accuracy: 0.9581 - val_loss: 0.1471 - val_accuracy: 0.9611\n","Epoch 11/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1493 - accuracy: 0.9579 - val_loss: 0.1480 - val_accuracy: 0.9592\n","Epoch 12/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1496 - accuracy: 0.9574 - val_loss: 0.1516 - val_accuracy: 0.9582\n","Epoch 13/25\n","294/294 [==============================] - 78s 265ms/step - loss: 0.1470 - accuracy: 0.9595 - val_loss: 0.1425 - val_accuracy: 0.9609\n","Epoch 14/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1449 - accuracy: 0.9590 - val_loss: 0.1411 - val_accuracy: 0.9607\n","Epoch 15/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1462 - accuracy: 0.9581 - val_loss: 0.1406 - val_accuracy: 0.9631\n","Epoch 16/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1456 - accuracy: 0.9579 - val_loss: 0.1414 - val_accuracy: 0.9620\n","Epoch 17/25\n","294/294 [==============================] - 77s 264ms/step - loss: 0.1402 - accuracy: 0.9587 - val_loss: 0.1483 - val_accuracy: 0.9599\n","Epoch 18/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1403 - accuracy: 0.9592 - val_loss: 0.1390 - val_accuracy: 0.9629\n","Epoch 19/25\n","294/294 [==============================] - 77s 264ms/step - loss: 0.1402 - accuracy: 0.9599 - val_loss: 0.1397 - val_accuracy: 0.9631\n","Epoch 20/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1394 - accuracy: 0.9605 - val_loss: 0.1484 - val_accuracy: 0.9590\n","Epoch 21/25\n","294/294 [==============================] - 77s 264ms/step - loss: 0.1398 - accuracy: 0.9592 - val_loss: 0.1559 - val_accuracy: 0.9544\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SxkyU_-3nyh","executionInfo":{"status":"ok","timestamp":1635330061321,"user_tz":-480,"elapsed":165694,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"f1c5c2b2-3ab7-4325-c60c-2ef36535771a"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 45s 124ms/step - loss: 0.1581 - accuracy: 0.9536\n","Test set\n","  Loss: 0.158\n","  Accuracy: 0.954\n","Precision is:  0.955613322188781\n","Recall is:  0.9536015292379877\n","F1 Score is:  0.9536323512888436\n"]}]},{"cell_type":"markdown","metadata":{"id":"haHZJe4nwtFK"},"source":["# LSTM + Skip-Gram\n","\n","- Choose where you want to save the embedding_word2vec txt files.\n","- You will be retrieving from that directory in the next code!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Go5WIZYnC1BO","executionInfo":{"status":"ok","timestamp":1635330192460,"user_tz":-480,"elapsed":58069,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"924be165-ca49-45da-d93e-317cf1191dae"},"source":["\"\"\"\n","Skip-Gram Model\n","\"\"\"\n","text_sentences = df['text'].apply(lambda x: x.split())\n","\n","model = gensim.models.Word2Vec(sentences=text_sentences, size=EMBEDDING_DIM, window=5, workers=4, min_count=1, sg=1)\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","# Where you want to save.\n","filename = 'depression_embedding_word2vec_skipgram.txt'\n","model.wv.save_word2vec_format(filename, binary=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53756\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4k_GlCiDkeA","executionInfo":{"status":"ok","timestamp":1635330296225,"user_tz":-480,"elapsed":7621,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"b446ed0c-0e73-41e0-df49-d698c6476351"},"source":["# SkipGram word2vec\n","import numpy as np\n","embeddings_index = {}\n","f = open(\"/content/depression_embedding_word2vec_skipgram.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","word2vec_embedding_matrix = np.zeros((num_words, 128))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    word2vec_embedding_matrix[i] = embedding_vector \n","print(num_words)\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'].values)\n","X = tokenizer.texts_to_sequences(df['text'].values)\n","\n","word_index = tokenizer.word_index\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","Y = pd.get_dummies(df['depression']).values\n","print('Shape of label tensor:', Y.shape)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 99)\n","# X_train, X_test, Y_train, Y_test = train_test_split(X_svd, Y, test_size = 0.30, random_state = 42)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53661\n","Shape of label tensor: (38362, 2)\n","(26853, 2521) (26853, 2)\n","(11509, 2521) (11509, 2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUHD0rnUDkgQ","executionInfo":{"status":"ok","timestamp":1635331853058,"user_tz":-480,"elapsed":1551242,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"aaaa59ce-fb5b-4bd1-f2a7-5097414fc3ef"},"source":["from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 128, weights=[word2vec_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 81s 266ms/step - loss: 0.4301 - accuracy: 0.8851 - val_loss: 0.3180 - val_accuracy: 0.9103\n","Epoch 2/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.2402 - accuracy: 0.9434 - val_loss: 0.2074 - val_accuracy: 0.9546\n","Epoch 3/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.2117 - accuracy: 0.9471 - val_loss: 0.1914 - val_accuracy: 0.9539\n","Epoch 4/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1970 - accuracy: 0.9490 - val_loss: 0.1798 - val_accuracy: 0.9549\n","Epoch 5/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1861 - accuracy: 0.9513 - val_loss: 0.1728 - val_accuracy: 0.9566\n","Epoch 6/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1782 - accuracy: 0.9543 - val_loss: 0.1644 - val_accuracy: 0.9590\n","Epoch 7/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1742 - accuracy: 0.9518 - val_loss: 0.1645 - val_accuracy: 0.9584\n","Epoch 8/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1707 - accuracy: 0.9523 - val_loss: 0.1638 - val_accuracy: 0.9568\n","Epoch 9/25\n","294/294 [==============================] - 77s 262ms/step - loss: 0.1661 - accuracy: 0.9531 - val_loss: 0.1523 - val_accuracy: 0.9604\n","Epoch 10/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1628 - accuracy: 0.9546 - val_loss: 0.1483 - val_accuracy: 0.9625\n","Epoch 11/25\n","294/294 [==============================] - 77s 262ms/step - loss: 0.1584 - accuracy: 0.9541 - val_loss: 0.1521 - val_accuracy: 0.9598\n","Epoch 12/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1566 - accuracy: 0.9567 - val_loss: 0.1490 - val_accuracy: 0.9592\n","Epoch 13/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1569 - accuracy: 0.9554 - val_loss: 0.1440 - val_accuracy: 0.9616\n","Epoch 14/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1514 - accuracy: 0.9552 - val_loss: 0.1430 - val_accuracy: 0.9600\n","Epoch 15/25\n","294/294 [==============================] - 78s 264ms/step - loss: 0.1516 - accuracy: 0.9561 - val_loss: 0.1399 - val_accuracy: 0.9638\n","Epoch 16/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1503 - accuracy: 0.9567 - val_loss: 0.1424 - val_accuracy: 0.9619\n","Epoch 17/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1497 - accuracy: 0.9566 - val_loss: 0.1370 - val_accuracy: 0.9629\n","Epoch 18/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1487 - accuracy: 0.9561 - val_loss: 0.1391 - val_accuracy: 0.9619\n","Epoch 19/25\n","294/294 [==============================] - 77s 262ms/step - loss: 0.1486 - accuracy: 0.9550 - val_loss: 0.1425 - val_accuracy: 0.9594\n","Epoch 20/25\n","294/294 [==============================] - 77s 263ms/step - loss: 0.1441 - accuracy: 0.9578 - val_loss: 0.1403 - val_accuracy: 0.9604\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAJOR3hADki4","executionInfo":{"status":"ok","timestamp":1635332051645,"user_tz":-480,"elapsed":164800,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"193d4bc2-1d2c-45a6-97b5-a32ca195e8cf"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 48s 134ms/step - loss: 0.1384 - accuracy: 0.9610\n","Test set\n","  Loss: 0.138\n","  Accuracy: 0.961\n","Precision is:  0.9618126254414924\n","Recall is:  0.9609870536102181\n","F1 Score is:  0.9610126628791377\n"]}]},{"cell_type":"markdown","metadata":{"id":"7Wc1-VNFwxIR"},"source":["# LSTM + Glove\n","\n","- glove.6B.300d.txt is attached in the submission folder. Please use the appropriate directory you stored it in."]},{"cell_type":"code","metadata":{"id":"Z7IIMPlE0yJP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635332351179,"user_tz":-480,"elapsed":37156,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"76be7a4b-8ebf-4c7d-e32d-09438aaedf57"},"source":["# Glove\n","import numpy as np\n","embeddings_index = {}\n","\n","# Use your own directory.\n","f = open(\"/content/drive/MyDrive/Colab Notebooks/IS424 Depression Project/Classification/glove.6B.300d.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'].values)\n","X = tokenizer.texts_to_sequences(df['text'].values)\n","\n","word_index = tokenizer.word_index\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","Y = pd.get_dummies(df['depression']).values\n","print('Shape of label tensor:', Y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53661\n","Shape of label tensor: (38362, 2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Id9jUioI2rZ-","executionInfo":{"status":"ok","timestamp":1635332364845,"user_tz":-480,"elapsed":276,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"f259bfa6-2831-4539-c1d8-1966f066438b"},"source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 99)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape,Y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(26853, 2521) (26853, 2)\n","(11509, 2521) (11509, 2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0I9jBkF61VE5","executionInfo":{"status":"ok","timestamp":1635333688050,"user_tz":-480,"elapsed":1307808,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"bb4f29eb-a372-4492-be15-3fd60c9e68e0"},"source":["from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 85s 281ms/step - loss: 0.4398 - accuracy: 0.8854 - val_loss: 0.2614 - val_accuracy: 0.9393\n","Epoch 2/25\n","294/294 [==============================] - 82s 279ms/step - loss: 0.2492 - accuracy: 0.9387 - val_loss: 0.2128 - val_accuracy: 0.9518\n","Epoch 3/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.2229 - accuracy: 0.9438 - val_loss: 0.2097 - val_accuracy: 0.9466\n","Epoch 4/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.2045 - accuracy: 0.9481 - val_loss: 0.1868 - val_accuracy: 0.9541\n","Epoch 5/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.1997 - accuracy: 0.9474 - val_loss: 0.1836 - val_accuracy: 0.9552\n","Epoch 6/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.1868 - accuracy: 0.9508 - val_loss: 0.1770 - val_accuracy: 0.9553\n","Epoch 7/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.1802 - accuracy: 0.9540 - val_loss: 0.1733 - val_accuracy: 0.9573\n","Epoch 8/25\n","294/294 [==============================] - 81s 277ms/step - loss: 0.1763 - accuracy: 0.9523 - val_loss: 0.1651 - val_accuracy: 0.9589\n","Epoch 9/25\n","294/294 [==============================] - 81s 277ms/step - loss: 0.1719 - accuracy: 0.9546 - val_loss: 0.1686 - val_accuracy: 0.9567\n","Epoch 10/25\n","294/294 [==============================] - 81s 277ms/step - loss: 0.1663 - accuracy: 0.9588 - val_loss: 0.1653 - val_accuracy: 0.9573\n","Epoch 11/25\n","294/294 [==============================] - 81s 277ms/step - loss: 0.1658 - accuracy: 0.9569 - val_loss: 0.1586 - val_accuracy: 0.9611\n","Epoch 12/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1618 - accuracy: 0.9570 - val_loss: 0.1580 - val_accuracy: 0.9589\n","Epoch 13/25\n","294/294 [==============================] - 81s 277ms/step - loss: 0.1592 - accuracy: 0.9579 - val_loss: 0.1519 - val_accuracy: 0.9636\n","Epoch 14/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1574 - accuracy: 0.9611 - val_loss: 0.1588 - val_accuracy: 0.9597\n","Epoch 15/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1544 - accuracy: 0.9600 - val_loss: 0.1532 - val_accuracy: 0.9605\n","Epoch 16/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1534 - accuracy: 0.9602 - val_loss: 0.1535 - val_accuracy: 0.9600\n"]}]},{"cell_type":"code","metadata":{"id":"23jPpp0_1VLD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635333791315,"user_tz":-480,"elapsed":64634,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"2418db62-9f17-46da-9739-5a6bcf17dc24"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 34s 94ms/step - loss: 0.1568 - accuracy: 0.9598\n","Test set\n","  Loss: 0.157\n","  Accuracy: 0.960\n","Precision is:  0.9601075235415188\n","Recall is:  0.9597706143018507\n","F1 Score is:  0.9597907993001087\n"]}]},{"cell_type":"markdown","metadata":{"id":"8k0if8M8w7QW"},"source":["# LSTM + Pre-trained Word2Vec"]},{"cell_type":"code","metadata":{"id":"tNwOaYYSh6ah"},"source":["\"\"\"\n","Pre-trained Word2Vec Model\n","This part loads really long, please be patient!\n","\"\"\"\n","\n","import gensim.downloader as api\n","model = api.load(\"word2vec-google-news-300\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09G6nBB5lkJa"},"source":["\"\"\"\n","Pre-trained Word2Vec Model\n","\"\"\"\n","\n","text_sentences = df['text'].apply(lambda x: x.split())\n","words = list(model.wv.vocab)\n","print(len(words))\n","\n","# filename = 'selfdriving_embedding_word2vec_pretrained.txt'\n","# model.wv.save_word2vec_format(filename, binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXLwaqOsiJ84","executionInfo":{"status":"ok","timestamp":1635335195706,"user_tz":-480,"elapsed":7632,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"fdc75e19-a3f6-4fcf-d75e-c98700dc311e"},"source":["import numpy as np\n","labels = np.asarray(model.index2word)\n","vectors = np.asarray(model.vectors)\n","word_embeddings = dict(zip(labels, vectors))\n","\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = word_embeddings.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'].values)\n","X = tokenizer.texts_to_sequences(df['text'].values)\n","\n","word_index = tokenizer.word_index\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","Y = pd.get_dummies(df['depression']).values\n","print('Shape of label tensor:', Y.shape)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 99)\n","# X_train, X_test, Y_train, Y_test = train_test_split(X_svd, Y, test_size = 0.30, random_state = 42)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53661\n","Shape of label tensor: (38362, 2)\n","(26853, 2521) (26853, 2)\n","(11509, 2521) (11509, 2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQX1ACaqmI5Y","executionInfo":{"status":"ok","timestamp":1635337293427,"user_tz":-480,"elapsed":1257431,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"e36ba7b5-a730-4bd0-d15d-2ecf41dfa575"},"source":["from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 89s 280ms/step - loss: 0.4425 - accuracy: 0.9022 - val_loss: 0.2597 - val_accuracy: 0.9458\n","Epoch 2/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.2574 - accuracy: 0.9402 - val_loss: 0.2242 - val_accuracy: 0.9525\n","Epoch 3/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.2306 - accuracy: 0.9464 - val_loss: 0.2024 - val_accuracy: 0.9526\n","Epoch 4/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.2167 - accuracy: 0.9467 - val_loss: 0.1883 - val_accuracy: 0.9556\n","Epoch 5/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.2049 - accuracy: 0.9483 - val_loss: 0.1808 - val_accuracy: 0.9561\n","Epoch 6/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1960 - accuracy: 0.9505 - val_loss: 0.1837 - val_accuracy: 0.9572\n","Epoch 7/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1887 - accuracy: 0.9515 - val_loss: 0.1807 - val_accuracy: 0.9532\n","Epoch 8/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1838 - accuracy: 0.9525 - val_loss: 0.1688 - val_accuracy: 0.9556\n","Epoch 9/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1770 - accuracy: 0.9523 - val_loss: 0.1756 - val_accuracy: 0.9535\n","Epoch 10/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1734 - accuracy: 0.9532 - val_loss: 0.1760 - val_accuracy: 0.9548\n","Epoch 11/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1704 - accuracy: 0.9530 - val_loss: 0.1558 - val_accuracy: 0.9616\n","Epoch 12/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1679 - accuracy: 0.9549 - val_loss: 0.1523 - val_accuracy: 0.9625\n","Epoch 13/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1642 - accuracy: 0.9548 - val_loss: 0.1534 - val_accuracy: 0.9595\n","Epoch 14/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1617 - accuracy: 0.9567 - val_loss: 0.1490 - val_accuracy: 0.9623\n","Epoch 15/25\n","294/294 [==============================] - 81s 276ms/step - loss: 0.1625 - accuracy: 0.9552 - val_loss: 0.1583 - val_accuracy: 0.9580\n","Epoch 16/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1583 - accuracy: 0.9554 - val_loss: 0.1477 - val_accuracy: 0.9619\n","Epoch 17/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1531 - accuracy: 0.9580 - val_loss: 0.1513 - val_accuracy: 0.9597\n","Epoch 18/25\n","294/294 [==============================] - 81s 275ms/step - loss: 0.1542 - accuracy: 0.9564 - val_loss: 0.1465 - val_accuracy: 0.9614\n","Epoch 19/25\n","294/294 [==============================] - 82s 277ms/step - loss: 0.1502 - accuracy: 0.9592 - val_loss: 0.1464 - val_accuracy: 0.9604\n","Epoch 20/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.1523 - accuracy: 0.9585 - val_loss: 0.1474 - val_accuracy: 0.9616\n","Epoch 21/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.1508 - accuracy: 0.9574 - val_loss: 0.1436 - val_accuracy: 0.9628\n","Epoch 22/25\n","294/294 [==============================] - 82s 278ms/step - loss: 0.1459 - accuracy: 0.9614 - val_loss: 0.1448 - val_accuracy: 0.9615\n","Epoch 23/25\n","294/294 [==============================] - 82s 279ms/step - loss: 0.1482 - accuracy: 0.9594 - val_loss: 0.1558 - val_accuracy: 0.9571\n","Epoch 24/25\n","294/294 [==============================] - 82s 280ms/step - loss: 0.1467 - accuracy: 0.9600 - val_loss: 0.1434 - val_accuracy: 0.9628\n","Epoch 25/25\n","294/294 [==============================] - 83s 284ms/step - loss: 0.1445 - accuracy: 0.9605 - val_loss: 0.1424 - val_accuracy: 0.9630\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5izROQ1KvgGU","executionInfo":{"status":"ok","timestamp":1635337498728,"user_tz":-480,"elapsed":83436,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"74aa1667-23a8-4db7-ccf6-01719b1c2762"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 37s 103ms/step - loss: 0.1426 - accuracy: 0.9632\n","Test set\n","  Loss: 0.143\n","  Accuracy: 0.963\n","Precision is:  0.9632444777731072\n","Recall is:  0.9632461551829004\n","F1 Score is:  0.9632449591810578\n"]}]},{"cell_type":"markdown","metadata":{"id":"a2184TsGtxjJ"},"source":["# Bi-LSTM + Glove\n","- glove.6B.300d.txt is attached in the submission folder. Please use the appropriate directory you stored it in."]},{"cell_type":"code","metadata":{"id":"LPAA_RaDtv7H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636159019012,"user_tz":-480,"elapsed":40568,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"b43c7498-733d-45b0-c0ba-8ddcf6772ada"},"source":["# Glove\n","import numpy as np\n","embeddings_index = {}\n","\n","\n","f = open(\"/content/drive/MyDrive/Colab Notebooks/IS424 Depression Project/Finalised Submission Folder/glove.6B.300d.txt\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:])\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, 300))\n","\n","for word, i in word_index.items():\n","  if i > num_words: \n","    continue\n","\n","  embedding_vector = embeddings_index.get(word) \n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector \n","print(num_words)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'].values)\n","X = tokenizer.texts_to_sequences(df['text'].values)\n","\n","word_index = tokenizer.word_index\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","Y = pd.get_dummies(df['depression']).values\n","print('Shape of label tensor:', Y.shape)\n","\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 99)\n","# X_train, X_test, Y_train, Y_test = train_test_split(X_svd, Y, test_size = 0.30, random_state = 42)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["53661\n","Shape of label tensor: (38362, 2)\n","(26853, 2521) (26853, 2)\n","(11509, 2521) (11509, 2)\n"]}]},{"cell_type":"code","metadata":{"id":"a0_HdsBQt2VA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635412598289,"user_tz":-480,"elapsed":3670524,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"1b1a51da-dfd0-4c9f-ea88-2cdd31a51396"},"source":["from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf \n","from tensorflow import keras\n","from keras.regularizers import l2\n","\n","model = Sequential()\n","adam = tf.optimizers.Adam(learning_rate=0.001)\n","model.add(Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n","                    trainable=False))\n","model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.5, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001))))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10,activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","\n","model.compile(optimizer=adam, loss=\"binary_crossentropy\", \n","     metrics=['accuracy'])\n","\n","epochs = 25\n","batch_size = 64\n","\n","\t\n","es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=[es])\n","# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","294/294 [==============================] - 175s 566ms/step - loss: 0.5291 - accuracy: 0.8786 - val_loss: 0.2843 - val_accuracy: 0.9464\n","Epoch 2/25\n","294/294 [==============================] - 165s 563ms/step - loss: 0.2657 - accuracy: 0.9427 - val_loss: 0.2293 - val_accuracy: 0.9516\n","Epoch 3/25\n","294/294 [==============================] - 165s 563ms/step - loss: 0.2286 - accuracy: 0.9483 - val_loss: 0.2082 - val_accuracy: 0.9525\n","Epoch 4/25\n","294/294 [==============================] - 165s 562ms/step - loss: 0.2173 - accuracy: 0.9461 - val_loss: 0.1984 - val_accuracy: 0.9544\n","Epoch 5/25\n","294/294 [==============================] - 165s 562ms/step - loss: 0.1967 - accuracy: 0.9530 - val_loss: 0.1919 - val_accuracy: 0.9530\n","Epoch 6/25\n","294/294 [==============================] - 166s 566ms/step - loss: 0.1894 - accuracy: 0.9525 - val_loss: 0.1798 - val_accuracy: 0.9559\n","Epoch 7/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1813 - accuracy: 0.9545 - val_loss: 0.1728 - val_accuracy: 0.9597\n","Epoch 8/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1753 - accuracy: 0.9559 - val_loss: 0.1672 - val_accuracy: 0.9599\n","Epoch 9/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1722 - accuracy: 0.9558 - val_loss: 0.1696 - val_accuracy: 0.9589\n","Epoch 10/25\n","294/294 [==============================] - 167s 567ms/step - loss: 0.1720 - accuracy: 0.9582 - val_loss: 0.1684 - val_accuracy: 0.9577\n","Epoch 11/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1669 - accuracy: 0.9579 - val_loss: 0.1658 - val_accuracy: 0.9573\n","Epoch 12/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1641 - accuracy: 0.9578 - val_loss: 0.1777 - val_accuracy: 0.9526\n","Epoch 13/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1665 - accuracy: 0.9564 - val_loss: 0.1605 - val_accuracy: 0.9603\n","Epoch 14/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1595 - accuracy: 0.9590 - val_loss: 0.1555 - val_accuracy: 0.9623\n","Epoch 15/25\n","294/294 [==============================] - 167s 568ms/step - loss: 0.1576 - accuracy: 0.9613 - val_loss: 0.1596 - val_accuracy: 0.9585\n","Epoch 16/25\n","294/294 [==============================] - 167s 567ms/step - loss: 0.1584 - accuracy: 0.9599 - val_loss: 0.1551 - val_accuracy: 0.9628\n","Epoch 17/25\n","294/294 [==============================] - 167s 567ms/step - loss: 0.1557 - accuracy: 0.9617 - val_loss: 0.1538 - val_accuracy: 0.9600\n","Epoch 18/25\n","294/294 [==============================] - 167s 567ms/step - loss: 0.1525 - accuracy: 0.9623 - val_loss: 0.1590 - val_accuracy: 0.9599\n","Epoch 19/25\n","294/294 [==============================] - 167s 567ms/step - loss: 0.1535 - accuracy: 0.9620 - val_loss: 0.1537 - val_accuracy: 0.9604\n","Epoch 20/25\n","294/294 [==============================] - 166s 566ms/step - loss: 0.1510 - accuracy: 0.9626 - val_loss: 0.1564 - val_accuracy: 0.9613\n","Epoch 21/25\n","294/294 [==============================] - 166s 566ms/step - loss: 0.1471 - accuracy: 0.9641 - val_loss: 0.1541 - val_accuracy: 0.9626\n","Epoch 22/25\n","294/294 [==============================] - 166s 566ms/step - loss: 0.1469 - accuracy: 0.9642 - val_loss: 0.1580 - val_accuracy: 0.9610\n"]}]},{"cell_type":"code","metadata":{"id":"6DbVxhvOt3H3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635413071305,"user_tz":-480,"elapsed":164462,"user":{"displayName":"NEO YONG YI, DARREN _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16409314409005165357"}},"outputId":"aca72c5e-829e-44ca-be59-4fd5059d0731"},"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","predictions = model.predict(X_test)\n","predictions = [np.argmax(prediction) for prediction in predictions]\n","precision, recall, f1_score, none = precision_recall_fscore_support(Y_test_arr, predictions, average='weighted')\n","print(\"Precision is: \", precision)\n","print(\"Recall is: \", recall)\n","print(\"F1 Score is: \", f1_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["360/360 [==============================] - 67s 186ms/step - loss: 0.1572 - accuracy: 0.9614\n","Test set\n","  Loss: 0.157\n","  Accuracy: 0.961\n","Precision is:  0.9614523505560858\n","Recall is:  0.9614214962203493\n","F1 Score is:  0.9614274979874109\n"]}]}]}